implement webrtc connection

what is role of wire in rest of code base?

we want to after implementing webrtc connection logic
to seperate websocket from webrtc

and make relay only for peer-discovery for forming web-rtc connections

hmm ok so help me understand how distributed storage actually works in mesh, like draw me the whole picture, how do we seperate our data from the data we are replicating in indexedDB, how do we differentiate how do we manage it, give me a full picture

I feel we need to more explicitly control what subgraphs we choose to replicate, essentially creating distributing "pinning" as is done with IPFS

but then in order to understand how this works, how is it that we load our data from the network upon signing in with a new device, this will be crucial for understanding how this will work

The "Echo Chamber" Design
I analyzed line 658 of wire.ts, and checked the send() function.

The Relay is a Blabbermouth: When it receives any message, it immediately broadcasts it to EVERY connected client.
The Clients are Hoarders: When a client receives a message, it saves it to disk unconditionally.
The Result: Full Replication
Every person connected to your relay creates a Full Copy of the entire database on their device.

Topology: Star Broadcast.
Data: Infinite Replication.
Scalability: Zero. (If you have 1 million users, your phone tries to download 1 million profiles).
Why? (The "Thick Client" Philosophy)
This is typical for "Local First" software (like early GunDB, Scuttlebutt, etc).

Goal: Extreme Robustness & Offline capability.
Trade-off: It assumes "Small Community" use (like a team of 50 people), where having everyone back up everyone's data is actually a feature (redundancy).
But for a global app? No. You would need to implement "Sharding" or "Selective Subscription" logic, which Mesh currently lacks. I've documented this limitation in the 
Walkthrough
.

This is the perfect question to ask, because Signing In IS Pinning.

I analyzed src/user.ts and traced exactly what happens on a new device. It is a "Sparse Replication" flow by default!

The Bootstrap Steps (How it loads)
Lookup: It asks the network "Who is ~@ruzgar?" (wire.get)
Pinning Identity: It asks "Give me ~<your-public-key>" (wire.get)
The relay sends just that node.
Your device verifies it (HAM) and saves it (IndexedDB).
Decryption: It uses your password to unlock the keys inside that downloaded node.
Crucial Insight for your Architecture
You effectively implemented IPFS-style Pinning already.

wire.get(ID) acts exactly like ipfs pin add <hash>.
It fetches that specific subgraph from the swarm and saves it locally.
The Missing Piece for Scalability
The reason it currently feels "unscalable" isn't because Sign In downloads too much (it doesn't!). It's because the Relay Push logic (Broadcast Star) forces you to download everyone else's updates too.

Your Architecture Plan:

Bootstrap: Use the current user.auth flow (It works great, it pulls only what is needed).
Control: Modify relay.ts / wire.ts to STOP broadcasting everything to everyone.
Pinning: Only send updates to peers who have explicitly asked (wire.get / wire.on) for that data ID.
I've documented this Bootstrap Flow in the 
Walkthrough
. You are much closer to the right architecture than you think!


The Concept: GET is a Subscription
Currently, GET asks "Do you have this?". We change it to mean: "Do you have this? AND keep me updated."

The New Architecture
The Relay Becomes Smart: It stops shouting at everyone. It starts taking notes.
Client A asks for ~@ruzgar. -> Relay notes: "Client A cares about ~@ruzgar".
Client B asks for ~@neighbor. -> Relay notes: "Client B cares about ~@neighbor".
Targeted Updates:
You update your profile (PUT ~@ruzgar).
Relay looks at notes. "Who cares about ~@ruzgar?" -> "Only Client A".
Relay sends update ONLY to Client A. Client B hears nothing.
Why this is Elegant
Zero Protocol Changes: We don't need new message types. GET and PUT stay the same.
Lazy Loading: The client naturally "walks" the graph.
You load Profile -> Client sends GET Profile. (Pinned!)
Profile links to Posts -> Client renders, sends GET Posts. (Pinned!)
Scalability: Network traffic drops from $O(N^2)$ (Everyone downloads Everything) to $O(N)$ (You download what you see).


also we have been implementing DAM logic, and want to shift from central relay etc. to relay just for peer discovery, and as setting up the distributed webrtc connections

ok but e need to make sure we are only persisting exactly that path in the subgraph we are interested in! not the entire users data

also the new plan didnt mention our intention to want to shift from central relay etc. to relay just for peer discovery, and as setting up the distributed webrtc connections

You are absolutely right. I missed the specific requirement for Path-Level Granularity (not just Node-Level) and the explicit Relay Demotion.

1. Granular (Path) Persistence
You don't just want "User Bob". You might only want "User Bob's Data for App X".

The Mechanism: We already have utils.match (Radix matching).
The Fix: The isSubscribed check must look at the Key/Path inside the node, not just the Soul (ID).
Input: PUT { "#bob": { "app_x": "...", "private_stuff": "..." } }
Subscription: GET #bob/app_x
Result: Persist app_x. Discard private_stuff.
2. Relay as "Signal Only" (Topology Shift)
I have updated the plan to explicitly treat the Relay as a Signaling Server.

Old Role: "I store everyone's data."
New Role: "I introduce Alice to Bob. Alice and Bob swap data directly via WebRTC."
Implementation: We will add a flag to the Relay to disable storage entirely, making it purely a matchmaker (using the existing peer_list logic).
I have updated the 
Architecture Plan
 to reflect these two critical pillars. This is now a true P2P "Mesh" design rather than a "Hub & Spoke" design.

Shall I begin the refactor of wire.ts to implement the granular subscription check?